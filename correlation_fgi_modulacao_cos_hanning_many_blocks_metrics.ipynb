{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b431a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy numpy matplotlib pyyaml pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8e5eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49d14e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amostras por chip (CLEAN): 25\n",
      "Amostras por chip (DS2): 25\n"
     ]
    }
   ],
   "source": [
    "def get_params(config):\n",
    "    try:\n",
    "        # Acessa as configura√ß√µes aninhadas\n",
    "        ds = config['DATASET']\n",
    "        rf = config['RF_FRONT_END']\n",
    "        gps = config['GPS_STANDARD']\n",
    "        acq = config['ACQUISITION']\n",
    "\n",
    "        # 1. Extra√ß√£o de Vari√°veis Base (Convers√£o de Tipo)\n",
    "        \n",
    "        FILENAME = ds['FILENAME']\n",
    "        # Converte a string 'int8' para o tipo numpy.int8\n",
    "        DATA_DTYPE = getattr(np, ds['DTYPE']) \n",
    "        PRN_ID_TO_SEARCH = ds['PRN_ID_TO_SEARCH']\n",
    "        \n",
    "        FS = rf['FS']\n",
    "        FIF = rf['FIF']\n",
    "        \n",
    "        PRN_CHIP_RATE = gps['PRN_CHIP_RATE']\n",
    "        PRN_LENGTH = gps['PRN_LENGTH']\n",
    "        \n",
    "        DOPPLER_RANGE = acq['DOPPLER_RANGE']\n",
    "        DOPPLER_STEP = acq['DOPPLER_STEP']\n",
    "        TIME_TO_PROCESS = acq['TIME_TO_PROCESS']\n",
    "\n",
    "        # 2. C√°lculo de Par√¢metros Derivados\n",
    "        \n",
    "        # Amostras por Chip (define o superamostragem da r√©plica do c√≥digo)\n",
    "        SAMPLES_PER_CHIP = round(FS / PRN_CHIP_RATE)\n",
    "\n",
    "        # Alguns arquivos de configura√ß√£o podem fornecer PRN_LENGTH j√° em amostras\n",
    "        # (ex: 16368 = 1023 chips * 16 amostras/chip). Detectamos esse caso e\n",
    "        # convertemos para n√∫mero de chips para manter consist√™ncia interna.\n",
    "        raw_prn_length = PRN_LENGTH\n",
    "        if raw_prn_length > 2000:\n",
    "            PRN_LENGTH = int(raw_prn_length // SAMPLES_PER_CHIP)\n",
    "\n",
    "        # Amostras em um Per√≠odo de C√≥digo (em amostras)\n",
    "        SAMPLES_PER_CODE = PRN_LENGTH * SAMPLES_PER_CHIP\n",
    "\n",
    "        # N√∫mero de Amostras para Integra√ß√£o Coerente (idealmente 1ms ou m√∫ltiplo)\n",
    "        N_SAMPLES_COHERENT = int(FS * TIME_TO_PROCESS)\n",
    "\n",
    "        # Garante que o bloco de amostras seja um m√∫ltiplo exato do per√≠odo do c√≥digo,\n",
    "        # essencial para a correla√ß√£o c√≠clica (FFT).\n",
    "        N_SAMPLES_COHERENT = (N_SAMPLES_COHERENT // SAMPLES_PER_CODE) * SAMPLES_PER_CODE\n",
    "\n",
    "        # Se o bloco calculado for 0 (por exemplo TIME_TO_PROCESS muito curto),\n",
    "        # fa√ßa fallback para processar ao menos um per√≠odo de c√≥digo (evita FFT de tamanho 0)\n",
    "        if N_SAMPLES_COHERENT == 0:\n",
    "            N_SAMPLES_COHERENT = SAMPLES_PER_CODE\n",
    "\n",
    "        # 3. Retorno das Constantes\n",
    "        return {\n",
    "            'FILENAME': FILENAME, \n",
    "            'DTYPE': DATA_DTYPE, \n",
    "            'PRN_ID_TO_SEARCH': PRN_ID_TO_SEARCH,\n",
    "            'FS': FS, \n",
    "            'FIF': FIF, \n",
    "            'PRN_CHIP_RATE': PRN_CHIP_RATE, \n",
    "            'PRN_LENGTH': PRN_LENGTH,\n",
    "            'DOPPLER_RANGE': DOPPLER_RANGE, \n",
    "            'DOPPLER_STEP': DOPPLER_STEP, \n",
    "            'TIME_TO_PROCESS': TIME_TO_PROCESS,\n",
    "            'SAMPLES_PER_CHIP': SAMPLES_PER_CHIP,\n",
    "            'SAMPLES_PER_CODE': SAMPLES_PER_CODE,\n",
    "            'N_SAMPLES_COHERENT': N_SAMPLES_COHERENT\n",
    "        }\n",
    "    \n",
    "    except KeyError as e:\n",
    "        # Captura erros se uma chave essencial n√£o estiver no arquivo de configura√ß√£o\n",
    "        raise KeyError(f\"Erro: Chave de configura√ß√£o ausente ou incorreta: {e}\")\n",
    "    except Exception as e:\n",
    "        # Captura erros gerais, como falha na convers√£o de tipo (ex: 'FS' n√£o √© um n√∫mero)\n",
    "        raise Exception(f\"Erro ao processar constantes: {e}\")\n",
    "\n",
    "def load_config(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'r') as f:\n",
    "            # Usa safe_load para evitar a execu√ß√£o de c√≥digo arbitr√°rio\n",
    "            config = yaml.safe_load(f)\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Erro: Arquivo de configura√ß√£o '{config_file}' n√£o encontrado.\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Erro ao decodificar YAML: {e}\")\n",
    "\n",
    "CONFIG_CLEAN_PATH = os.path.join(\"config\", \"fgi\", \"config_clean.yaml\")\n",
    "CONFIG_DS2_PATH = os.path.join(\"config\", \"fgi\", \"config_spoofed.yaml\")\n",
    "\n",
    "config_clean_data = load_config(CONFIG_CLEAN_PATH)\n",
    "config_ds2_data = load_config(CONFIG_DS2_PATH)\n",
    "\n",
    "PARAMS_CLEAN = get_params(config_clean_data)\n",
    "PARAMS_DS2 = get_params(config_ds2_data)\n",
    "\n",
    "print(f\"Amostras por chip (CLEAN): {PARAMS_CLEAN['SAMPLES_PER_CHIP']}\")\n",
    "print(f\"Amostras por chip (DS2): {PARAMS_DS2['SAMPLES_PER_CHIP']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80e300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desmodulate(gps_complex, N_samples, Fs, Fif_local):\n",
    "    \"\"\"\n",
    "    Move o sinal para banda base usando a frequ√™ncia local Fif_local.\n",
    "    \"\"\"\n",
    "    t = np.arange(N_samples) / Fs\n",
    "    local_carrier = np.cos(-2 * np.pi * Fif_local * t)\n",
    "    # local_carrier = np.exp(-1j * 2 * np.pi * Fif_local * t)\n",
    "    return gps_complex * local_carrier\n",
    "# --- 1. Mapeamento dos Taps G2 (Baseado no seu c√≥digo) ---\n",
    "# O √≠ndice da linha corresponde ao PRN ID (1 a 37).\n",
    "# Os valores nos vetores s√£o os est√°gios do G2 que s√£o combinados com G1.\n",
    "PRN_G2_TAPS = [\n",
    "    [2, 6], [3, 7], [4, 8], [5, 9], [1, 9], [2, 10], [1, 8], [2, 9], \n",
    "    [3, 10], [2, 3], [3, 4], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], \n",
    "    [1, 4], [2, 5], [3, 6], [4, 7], [5, 8], [6, 9], [1, 3], [4, 6], \n",
    "    [5, 7], [6, 8], [7, 9], [8, 10], [1, 6], [2, 7], [3, 8], [4, 9], \n",
    "    [5, 10], [4, 10], [1, 7], [2, 8], [4, 10]\n",
    "]\n",
    "\n",
    "def generate_ca_code(prn_id, samples_per_chip, code_length=1023):\n",
    "    sv_index = prn_id - 1\n",
    "\n",
    "    if prn_id < 1 or prn_id > len(PRN_G2_TAPS):\n",
    "        raise ValueError(\"PRN ID inv√°lido (fora do intervalo 1-37).\")\n",
    "\n",
    "    # Inicializa√ß√£o: G1 e G2 com todos os 1s (usando 0/1 para l√≥gica bin√°ria)\n",
    "    LFSR_LEN = 10\n",
    "    g1 = np.ones(LFSR_LEN, dtype=int)\n",
    "    g2 = np.ones(LFSR_LEN, dtype=int)\n",
    "\n",
    "    ca_code_chips = np.zeros(code_length, dtype=int)\n",
    "\n",
    "    # Taps G2 espec√≠ficas do sat√©lite (posi√ß√µes de 1 a 10)\n",
    "    g2_taps = PRN_G2_TAPS[sv_index]\n",
    "\n",
    "    for i in range(code_length):\n",
    "        # Sa√≠da dos registradores\n",
    "        out_g1 = g1[9]\n",
    "        out_g2 = g2[g2_taps[0] - 1] ^ g2[g2_taps[1] - 1]\n",
    "\n",
    "        ca_chip_binary = out_g1 ^ out_g2\n",
    "        ca_code_chips[i] = ca_chip_binary\n",
    "\n",
    "        # Feedbacks\n",
    "        feedback_g1 = g1[2] ^ g1[9]\n",
    "        feedback_g2 = g2[1] ^ g2[2] ^ g2[5] ^ g2[7] ^ g2[8] ^ g2[9]\n",
    "\n",
    "        g1 = np.roll(g1, 1)\n",
    "        g1[0] = feedback_g1\n",
    "\n",
    "        g2 = np.roll(g2, 1)\n",
    "        g2[0] = feedback_g2\n",
    "\n",
    "    # Convers√£o de chips bin√°rios (0, 1) para bipolares (-1, +1)\n",
    "    bipolar_code_chips = 2 * ca_code_chips - 1\n",
    "\n",
    "    # superamostragem dos chips\n",
    "    bipolar_code_chips = np.repeat(bipolar_code_chips, samples_per_chip)\n",
    "\n",
    "    # Retorna os chips na taxa de chip (sem repeti√ß√£o)\n",
    "    return bipolar_code_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd5b62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_correlation_by_noise(correlation_magnitude, samples_per_code, prn_length):\n",
    "    # A. Acha a posi√ß√£o exata e o valor do pico bruto\n",
    "    phase = np.argmax(correlation_magnitude)\n",
    "    peak_value_raw = correlation_magnitude[phase]\n",
    "\n",
    "    # B. Define a largura de exclus√£o ao redor do pico (1.5 chips)\n",
    "    # samples_per_code / prn_length √© o mesmo que samples_per_chip.\n",
    "    samples_per_chip = samples_per_code / prn_length\n",
    "    exclude_samples = int(1.5 * samples_per_chip)\n",
    "    \n",
    "    # Define os limites de exclus√£o\n",
    "    start = max(0, phase - exclude_samples)\n",
    "    end = min(len(correlation_magnitude), phase + exclude_samples)\n",
    "    \n",
    "    # C. Calcula o N√≠vel de Ru√≠do (Noise Floor)\n",
    "    \n",
    "    # Concatena os valores antes e depois da √°rea do pico\n",
    "    noise_values = np.concatenate([correlation_magnitude[:start], correlation_magnitude[end:]])\n",
    "    \n",
    "    # Usa a MEDIANA como medida robusta do ch√£o de ru√≠do (Noise Floor)\n",
    "    if len(noise_values) == 0:\n",
    "        # Fallback se o array for muito pequeno\n",
    "        noise_level = np.median(correlation_magnitude) \n",
    "    else:\n",
    "        noise_level = np.median(noise_values)\n",
    "\n",
    "    # D. Normaliza√ß√£o\n",
    "    if noise_level < 1e-9: # Evita divis√£o por zero ou por um n√∫mero muito pequeno\n",
    "        noise_level = 1e-9\n",
    "\n",
    "    corr_normalized = correlation_magnitude / noise_level\n",
    "\n",
    "    # E. Valor do pico normalizado (SNR do Pico)\n",
    "    peak_normalized = corr_normalized[phase]\n",
    "\n",
    "    return corr_normalized, peak_normalized, phase\n",
    "\n",
    "def correlate_signals(band_base_signal, local_code_replica):\n",
    "    N = len(band_base_signal)\n",
    "    window = np.hanning(N)\n",
    "    # 1. Transformar o sinal e a r√©plica do c√≥digo\n",
    "    fft_signal = np.fft.fft(band_base_signal)\n",
    "    fft_code_replica = np.fft.fft(local_code_replica)\n",
    "\n",
    "    # 2. Multiplica√ß√£o no dom√≠nio da frequ√™ncia (Correla√ß√£o)\n",
    "    fft_product = fft_signal * np.conjugate(fft_code_replica)\n",
    "    \n",
    "    # 3. IFFT e Magnitude\n",
    "    correlation_result = np.fft.ifft(fft_product)\n",
    "    \n",
    "    return np.abs(correlation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5a0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fourier_spectra(band_base_signal, local_code_replica, Fs):\n",
    "    \"\"\"\n",
    "    Calcula e plota o espectro de magnitude (FFT) do sinal em banda base \n",
    "    e da r√©plica do c√≥digo PRN.\n",
    "    \"\"\"\n",
    "    N = len(band_base_signal)\n",
    "    \n",
    "    # Aplicar janela de Hanning para suavizar os picos negativos (zeros espectrais)\n",
    "    window = np.hanning(N)\n",
    "    \n",
    "    # 1. C√°lculo da FFT\n",
    "    # A FFT do c√≥digo PRN deve ser conjugada no dom√≠nio da frequ√™ncia para a correla√ß√£o, \n",
    "    # mas para a plotagem do espectro, usamos a magnitude simples.\n",
    "    fft_signal = np.fft.fft(band_base_signal)\n",
    "    fft_code = np.fft.fft(local_code_replica * window)\n",
    "    \n",
    "    # 2. Deslocamento de Frequ√™ncia (FFT-shift)\n",
    "    # Move a frequ√™ncia zero (DC) para o centro do espectro.\n",
    "    fft_signal_shifted = np.fft.fftshift(fft_signal)\n",
    "    fft_code_shifted = np.fft.fftshift(fft_code)\n",
    "    \n",
    "    # 3. C√°lculo da Pot√™ncia (Magnitude ao Quadrado ou apenas Magnitude em dB)\n",
    "    # Usamos o logaritmo da magnitude (em dB) para visualizar melhor a faixa din√¢mica.\n",
    "    spectrum_signal = 10 * np.log10(np.abs(fft_signal_shifted) + 1e-10) # +1e-10 para evitar log(0)\n",
    "    spectrum_code = 10 * np.log10(np.abs(fft_code_shifted) + 1e-10)\n",
    "    \n",
    "    # 4. Gera√ß√£o do Eixo de Frequ√™ncia\n",
    "    # O eixo de frequ√™ncia vai de -Fs/2 at√© +Fs/2\n",
    "    freq_axis = np.fft.fftshift(np.fft.fftfreq(N, d=1/Fs))\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plotagem do Espectro do C√≥digo PRN\n",
    "    plt.plot(freq_axis / 1e6, spectrum_code, color='orange', label='Espectro da R√©plica do C√≥digo PRN')\n",
    "    \n",
    "    # Plotagem do Espectro do Sinal em Banda Base (com Ru√≠do)\n",
    "    plt.plot(freq_axis / 1e6, spectrum_signal, color='blue', label='Espectro do Sinal GNSS em Banda Base')\n",
    "\n",
    "    plt.title('Espectro de Frequ√™ncia dos Sinais (Dom√≠nio de Frequ√™ncia)')\n",
    "    plt.xlabel('Frequ√™ncia (MHz)')\n",
    "    plt.ylabel('Magnitude (dB)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "463ad58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_gnss_signal(filepath):\n",
    "\n",
    "    print(f\"\\nIniciando aquisi√ß√£o GNSS para o arquivo: {filepath}\\n\")\n",
    "\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    if \"clean\" in filename:\n",
    "        cfg = PARAMS_CLEAN\n",
    "    elif \"TGS\" in filename:\n",
    "        cfg = PARAMS_DS2\n",
    "\n",
    "    # --- Configura√ß√£o de processamento em blocos ---\n",
    "    BLOCK_TIME = 0.001  # 1 ms por bloco\n",
    "    TOTAL_TIME = cfg[\"TIME_TO_PROCESS\"]  # Tempo total a processar (ex: 120s)\n",
    "    \n",
    "    samples_per_block = int(cfg[\"FS\"] * BLOCK_TIME)\n",
    "    # Garante que seja m√∫ltiplo do per√≠odo do c√≥digo\n",
    "    samples_per_block = (samples_per_block // cfg[\"SAMPLES_PER_CODE\"]) * cfg[\"SAMPLES_PER_CODE\"]\n",
    "    \n",
    "    total_samples = int(cfg[\"FS\"] * TOTAL_TIME)\n",
    "    num_blocks = total_samples // samples_per_block\n",
    "    \n",
    "    print(f\"Processando {num_blocks} blocos de {BLOCK_TIME*1000:.1f} ms cada\")\n",
    "    print(f\"Tempo total: {TOTAL_TIME} segundos\")\n",
    "    print(f\"Amostras por bloco: {samples_per_block}\")\n",
    "\n",
    "    # --- 1. Leitura do arquivo ---\n",
    "    try:\n",
    "        print(f\"\\nLendo arquivo: {filepath}\")\n",
    "        raw_data = np.fromfile(filepath, dtype=cfg[\"DTYPE\"], count=total_samples)\n",
    "        \n",
    "        if len(raw_data) < total_samples:\n",
    "            print(f\"Aviso: Arquivo tem apenas {len(raw_data)} amostras ({len(raw_data)/cfg['FS']:.2f}s)\")\n",
    "            num_blocks = len(raw_data) // samples_per_block\n",
    "            print(f\"Ajustando para {num_blocks} blocos\")\n",
    "        \n",
    "        # Converter para float32\n",
    "        raw_data = raw_data.astype(np.float32)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo '{filepath}' n√£o encontrado.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Armazenar resultados de cada bloco ---\n",
    "    results = {\n",
    "        'block_idx': [],\n",
    "        'time_s': [],\n",
    "        'best_prn': [],\n",
    "        'best_doppler': [],\n",
    "        'best_tau_chips': [],\n",
    "        'peak_snr': [],\n",
    "        'peak_raw': [],\n",
    "        'in_phase_lag': [],  # Nova m√©trica: atraso de fase (diferen√ßa entre blocos)\n",
    "        'correlation_width': []  # Largura do pico de correla√ß√£o\n",
    "    }\n",
    "\n",
    "    # Gerar c√≥digo PRN uma vez (vamos buscar todos os PRNs)\n",
    "    prn_codes = {}\n",
    "    for prn in range(1, 33):\n",
    "        prn_codes[prn] = generate_ca_code(prn, cfg[\"SAMPLES_PER_CHIP\"], cfg[\"PRN_LENGTH\"])\n",
    "\n",
    "    doppler_freqs = np.arange(-cfg[\"DOPPLER_RANGE\"], cfg[\"DOPPLER_RANGE\"] + cfg[\"DOPPLER_STEP\"], cfg[\"DOPPLER_STEP\"])\n",
    "\n",
    "    # --- 3. Processar cada bloco ---\n",
    "    for block_idx in range(num_blocks):\n",
    "        start_sample = block_idx * samples_per_block\n",
    "        end_sample = start_sample + samples_per_block\n",
    "        \n",
    "        # Extrair bloco\n",
    "        block_data = raw_data[start_sample:end_sample]\n",
    "        gps_complex = block_data + 1j * np.zeros_like(block_data)\n",
    "        N_block = len(gps_complex)\n",
    "        \n",
    "        # Busca 2D para este bloco\n",
    "        max_correlation_value = 0.0\n",
    "        best_doppler = 0.0\n",
    "        best_tau = 0\n",
    "        best_prn = 0\n",
    "        best_code_replica = None\n",
    "        \n",
    "        for prn in range(1, 33):\n",
    "            local_code_chips = prn_codes[prn]\n",
    "            local_code_replica = np.tile(local_code_chips, N_block // cfg[\"SAMPLES_PER_CODE\"])\n",
    "            \n",
    "            for doppler in doppler_freqs:\n",
    "                Fif_local = cfg[\"FIF\"] + doppler\n",
    "                band_base_signal = desmodulate(gps_complex, N_block, cfg[\"FS\"], Fif_local)\n",
    "                \n",
    "                correlation_magnitude = correlate_signals(band_base_signal, local_code_replica)\n",
    "                current_max = np.max(correlation_magnitude)\n",
    "                \n",
    "                if current_max > max_correlation_value:\n",
    "                    max_correlation_value = current_max\n",
    "                    best_doppler = doppler\n",
    "                    best_tau = np.argmax(correlation_magnitude)\n",
    "                    best_prn = prn\n",
    "                    best_code_replica = local_code_replica.copy()\n",
    "        \n",
    "        # Calcular SNR normalizado\n",
    "        final_fif_local = cfg[\"FIF\"] + best_doppler\n",
    "        final_band_base = desmodulate(gps_complex, N_block, cfg[\"FS\"], final_fif_local)\n",
    "        final_correlation = correlate_signals(final_band_base, best_code_replica)\n",
    "        _, peak_snr, peak_phase = normalize_correlation_by_noise(final_correlation, cfg['SAMPLES_PER_CODE'], cfg['PRN_LENGTH'])\n",
    "        \n",
    "        # ============ NOVAS M√âTRICAS ============\n",
    "        \n",
    "        # 1. In-Phase Lag: diferen√ßa de fase entre blocos consecutivos\n",
    "        # Indica a taxa de varia√ß√£o do atraso de c√≥digo (deve ser suave para sinais leg√≠timos)\n",
    "        if block_idx > 0:\n",
    "            prev_tau = results['best_tau_chips'][-1]\n",
    "            current_tau = best_tau / cfg[\"SAMPLES_PER_CHIP\"]\n",
    "            # Tratar wrap-around (quando œÑ vai de ~1023 para ~0)\n",
    "            tau_diff = current_tau - prev_tau\n",
    "            if abs(tau_diff) > cfg['PRN_LENGTH'] / 2:\n",
    "                tau_diff = tau_diff - np.sign(tau_diff) * cfg['PRN_LENGTH']\n",
    "            in_phase_lag = tau_diff\n",
    "        else:\n",
    "            in_phase_lag = 0.0\n",
    "        \n",
    "        # 2. Correlation Width: largura do pico de correla√ß√£o (a -3dB ou 50% do pico)\n",
    "        # Picos mais estreitos indicam melhor qualidade do sinal\n",
    "        peak_value = final_correlation[peak_phase]\n",
    "        half_peak = peak_value * 0.5\n",
    "        # Encontrar pontos onde a correla√ß√£o cruza 50% do pico\n",
    "        above_half = final_correlation > half_peak\n",
    "        # Contar amostras acima de 50%\n",
    "        correlation_width = np.sum(above_half) / cfg[\"SAMPLES_PER_CHIP\"]  # Em chips\n",
    "        \n",
    "        # Armazenar resultados\n",
    "        time_s = block_idx * BLOCK_TIME\n",
    "        tau_chips = best_tau / cfg[\"SAMPLES_PER_CHIP\"];\n",
    "        \n",
    "        results['block_idx'].append(block_idx)\n",
    "        results['time_s'].append(time_s)\n",
    "        results['best_prn'].append(best_prn)\n",
    "        results['best_doppler'].append(best_doppler)\n",
    "        results['best_tau_chips'].append(tau_chips)\n",
    "        results['peak_snr'].append(peak_snr)\n",
    "        results['peak_raw'].append(max_correlation_value)\n",
    "        results['in_phase_lag'].append(in_phase_lag)\n",
    "        results['correlation_width'].append(correlation_width)\n",
    "        \n",
    "        # Progresso a cada 1000 blocos (1 segundo)\n",
    "        if (block_idx + 1) % 1000 == 0:\n",
    "            print(f\"Bloco {block_idx + 1}/{num_blocks} ({time_s:.2f}s) - PRN {best_prn}, Doppler {best_doppler:.0f} Hz, SNR {peak_snr:.2f}\")\n",
    "\n",
    "    # --- 4. Converter para arrays numpy ---\n",
    "    for key in results:\n",
    "        results[key] = np.array(results[key])\n",
    "\n",
    "    # --- 5. Plotagem dos resultados ao longo do tempo ---\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
    "    \n",
    "    # SNR ao longo do tempo\n",
    "    axes[0].plot(results['time_s'], results['peak_snr'], 'b-', linewidth=0.5)\n",
    "    axes[0].set_ylabel('SNR (normalizado)')\n",
    "    axes[0].set_title(f'Evolu√ß√£o Temporal - {filename}')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # PRN detectado\n",
    "    axes[1].plot(results['time_s'], results['best_prn'], 'g.', markersize=1)\n",
    "    axes[1].set_ylabel('PRN Detectado')\n",
    "    axes[1].set_ylim(0, 33)\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Doppler\n",
    "    axes[2].plot(results['time_s'], results['best_doppler'], 'r-', linewidth=0.5)\n",
    "    axes[2].set_ylabel('Doppler (Hz)')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    # Atraso de c√≥digo (tau)\n",
    "    axes[3].plot(results['time_s'], results['best_tau_chips'], 'm-', linewidth=0.5)\n",
    "    axes[3].set_ylabel('Atraso œÑ (chips)')\n",
    "    axes[3].set_xlabel('Tempo (s)')\n",
    "    axes[3].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 6. Estat√≠sticas ---\n",
    "    print(\"\\n--- Estat√≠sticas Gerais ---\")\n",
    "    print(f\"Blocos processados: {num_blocks}\")\n",
    "    print(f\"SNR m√©dio: {np.mean(results['peak_snr']):.2f}\")\n",
    "    print(f\"SNR m√°ximo: {np.max(results['peak_snr']):.2f}\")\n",
    "    print(f\"PRN mais frequente: {np.bincount(results['best_prn'].astype(int)).argmax()}\")\n",
    "    print(f\"Doppler m√©dio: {np.mean(results['best_doppler']):.1f} Hz\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7afc69ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fun√ß√µes de detec√ß√£o de spoofing carregadas!\n",
      "- calculate_spoofing_metrics(results, cfg)\n",
      "- print_spoofing_report(metrics, filename)\n",
      "- plot_spoofing_comparison(results_clean, results_spoofed, metrics_clean, metrics_spoofed)\n"
     ]
    }
   ],
   "source": [
    "def calculate_spoofing_metrics(results, cfg):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas para detec√ß√£o de spoofing GNSS.\n",
    "    \n",
    "    M√©tricas implementadas:\n",
    "    1. SNR Statistics - Sinais spoofed tendem a ter SNR mais alto e consistente\n",
    "    2. Doppler Consistency - Spoofing pode ter varia√ß√£o Doppler an√¥mala\n",
    "    3. Code Phase Consistency - Varia√ß√£o do atraso de c√≥digo\n",
    "    4. PRN Diversity - Quantos PRNs diferentes s√£o detectados\n",
    "    5. C/N0 Variance - Vari√¢ncia da rela√ß√£o sinal-ru√≠do\n",
    "    6. Multi-peak Detection - Detec√ß√£o de m√∫ltiplos picos (spoofing vs leg√≠timo)\n",
    "    7. Power Level Analysis - N√≠vel de pot√™ncia anormalmente alto\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # ============ 1. SNR Statistics ============\n",
    "    # Sinais spoofed geralmente t√™m SNR mais alto e mais est√°vel\n",
    "    snr_values = results['peak_snr']\n",
    "    metrics['snr_mean'] = np.mean(snr_values)\n",
    "    metrics['snr_std'] = np.std(snr_values)\n",
    "    metrics['snr_max'] = np.max(snr_values)\n",
    "    metrics['snr_min'] = np.min(snr_values)\n",
    "    metrics['snr_cv'] = metrics['snr_std'] / metrics['snr_mean'] if metrics['snr_mean'] > 0 else 0  # Coeficiente de varia√ß√£o\n",
    "    \n",
    "    # Threshold: SNR muito alto pode indicar spoofing\n",
    "    SNR_THRESHOLD_HIGH = 15.0  # Ajustar conforme necess√°rio\n",
    "    metrics['snr_above_threshold'] = np.sum(snr_values > SNR_THRESHOLD_HIGH) / len(snr_values) * 100\n",
    "    \n",
    "    # ============ 2. Doppler Consistency ============\n",
    "    # Doppler deve variar suavemente para sat√©lites reais\n",
    "    doppler_values = results['best_doppler']\n",
    "    metrics['doppler_mean'] = np.mean(doppler_values)\n",
    "    metrics['doppler_std'] = np.std(doppler_values)\n",
    "    \n",
    "    # Taxa de varia√ß√£o do Doppler (deve ser suave para sinais leg√≠timos)\n",
    "    doppler_diff = np.diff(doppler_values)\n",
    "    metrics['doppler_rate_mean'] = np.mean(np.abs(doppler_diff))\n",
    "    metrics['doppler_rate_max'] = np.max(np.abs(doppler_diff))\n",
    "    \n",
    "    # Saltos abruptos de Doppler (indicador de spoofing)\n",
    "    DOPPLER_JUMP_THRESHOLD = 500  # Hz\n",
    "    metrics['doppler_jumps'] = np.sum(np.abs(doppler_diff) > DOPPLER_JUMP_THRESHOLD)\n",
    "    metrics['doppler_jump_rate'] = metrics['doppler_jumps'] / len(doppler_diff) * 100\n",
    "    \n",
    "    # ============ 3. Code Phase (œÑ) Consistency ============\n",
    "    # O atraso de c√≥digo deve variar suavemente\n",
    "    tau_values = results['best_tau_chips']\n",
    "    metrics['tau_mean'] = np.mean(tau_values)\n",
    "    metrics['tau_std'] = np.std(tau_values)\n",
    "    \n",
    "    # Taxa de varia√ß√£o do œÑ\n",
    "    tau_diff = np.diff(tau_values)\n",
    "    # Tratar wrap-around (quando œÑ vai de ~1023 para ~0)\n",
    "    tau_diff = np.where(np.abs(tau_diff) > cfg['PRN_LENGTH']/2, \n",
    "                        tau_diff - np.sign(tau_diff) * cfg['PRN_LENGTH'], \n",
    "                        tau_diff)\n",
    "    metrics['tau_rate_mean'] = np.mean(np.abs(tau_diff))\n",
    "    metrics['tau_rate_std'] = np.std(tau_diff)\n",
    "    \n",
    "    # ============ 4. PRN Diversity ============\n",
    "    # Sinais leg√≠timos devem mostrar m√∫ltiplos PRNs vis√≠veis\n",
    "    prn_values = results['best_prn']\n",
    "    unique_prns = np.unique(prn_values)\n",
    "    metrics['prn_unique_count'] = len(unique_prns)\n",
    "    metrics['prn_unique_list'] = unique_prns.tolist()\n",
    "    \n",
    "    # PRN dominante e sua frequ√™ncia\n",
    "    prn_counts = np.bincount(prn_values.astype(int), minlength=33)\n",
    "    metrics['prn_dominant'] = np.argmax(prn_counts)\n",
    "    metrics['prn_dominant_ratio'] = prn_counts[metrics['prn_dominant']] / len(prn_values) * 100\n",
    "    \n",
    "    # ============ 5. Power Level Analysis ============\n",
    "    # Pot√™ncia bruta do pico de correla√ß√£o\n",
    "    power_values = results['peak_raw']\n",
    "    metrics['power_mean'] = np.mean(power_values)\n",
    "    metrics['power_std'] = np.std(power_values)\n",
    "    metrics['power_cv'] = metrics['power_std'] / metrics['power_mean'] if metrics['power_mean'] > 0 else 0\n",
    "    \n",
    "    # ============ 6. Temporal Correlation ============\n",
    "    # Autocorrela√ß√£o do SNR (sinais spoofed podem ter padr√µes artificiais)\n",
    "    if len(snr_values) > 100:\n",
    "        snr_normalized = (snr_values - np.mean(snr_values)) / (np.std(snr_values) + 1e-10)\n",
    "        autocorr = np.correlate(snr_normalized[:1000], snr_normalized[:1000], mode='full')\n",
    "        autocorr = autocorr[len(autocorr)//2:]  # Pegar apenas lag positivo\n",
    "        autocorr = autocorr / autocorr[0]  # Normalizar\n",
    "        metrics['snr_autocorr_lag1'] = autocorr[1] if len(autocorr) > 1 else 0\n",
    "        metrics['snr_autocorr_lag10'] = autocorr[10] if len(autocorr) > 10 else 0\n",
    "    else:\n",
    "        metrics['snr_autocorr_lag1'] = 0\n",
    "        metrics['snr_autocorr_lag10'] = 0\n",
    "    \n",
    "    # ============ 7. In-Phase Lag Analysis ============\n",
    "    # Varia√ß√£o do atraso de fase entre blocos consecutivos\n",
    "    in_phase_lag_values = results['in_phase_lag']\n",
    "    metrics['in_phase_lag_mean'] = np.mean(np.abs(in_phase_lag_values))\n",
    "    metrics['in_phase_lag_std'] = np.std(in_phase_lag_values)\n",
    "    metrics['in_phase_lag_max'] = np.max(np.abs(in_phase_lag_values))\n",
    "    \n",
    "    # Saltos abruptos de fase (indicador de spoofing)\n",
    "    PHASE_JUMP_THRESHOLD = 0.5  # chips\n",
    "    phase_jumps = np.sum(np.abs(in_phase_lag_values) > PHASE_JUMP_THRESHOLD)\n",
    "    metrics['phase_jumps'] = phase_jumps\n",
    "    metrics['phase_jump_rate'] = phase_jumps / len(in_phase_lag_values) * 100\n",
    "    \n",
    "    # ============ 8. Correlation Width Analysis ============\n",
    "    # Largura do pico de correla√ß√£o (picos muito estreitos ou largos podem indicar anomalias)\n",
    "    corr_width_values = results['correlation_width']\n",
    "    metrics['corr_width_mean'] = np.mean(corr_width_values)\n",
    "    metrics['corr_width_std'] = np.std(corr_width_values)\n",
    "    metrics['corr_width_cv'] = metrics['corr_width_std'] / metrics['corr_width_mean'] if metrics['corr_width_mean'] > 0 else 0\n",
    "    \n",
    "    # ============ 9. Temporal Correlation ============\n",
    "    # Autocorrela√ß√£o do SNR (sinais spoofed podem ter padr√µes artificiais)\n",
    "    if len(snr_values) > 100:\n",
    "        snr_normalized = (snr_values - np.mean(snr_values)) / (np.std(snr_values) + 1e-10)\n",
    "        autocorr = np.correlate(snr_normalized[:1000], snr_normalized[:1000], mode='full')\n",
    "        autocorr = autocorr[len(autocorr)//2:]  # Pegar apenas lag positivo\n",
    "        autocorr = autocorr / autocorr[0]  # Normalizar\n",
    "        metrics['snr_autocorr_lag1'] = autocorr[1] if len(autocorr) > 1 else 0\n",
    "        metrics['snr_autocorr_lag10'] = autocorr[10] if len(autocorr) > 10 else 0\n",
    "    else:\n",
    "        metrics['snr_autocorr_lag1'] = 0\n",
    "        metrics['snr_autocorr_lag10'] = 0\n",
    "    \n",
    "    # ============ 10. Spoofing Score (Combinado) ============\n",
    "    # Score baseado em m√∫ltiplos indicadores (0-100, maior = mais suspeito)\n",
    "    score = 0\n",
    "    \n",
    "    # SNR muito alto e est√°vel √© suspeito\n",
    "    if metrics['snr_mean'] > 10:\n",
    "        score += min(20, (metrics['snr_mean'] - 10) * 2)\n",
    "    if metrics['snr_cv'] < 0.1:  # Muito est√°vel\n",
    "        score += 10\n",
    "    \n",
    "    # Pouca diversidade de PRN √© suspeito\n",
    "    if metrics['prn_unique_count'] < 4:\n",
    "        score += 15\n",
    "    if metrics['prn_dominant_ratio'] > 90:\n",
    "        score += 10\n",
    "    \n",
    "    # Saltos de Doppler s√£o suspeitos\n",
    "    if metrics['doppler_jump_rate'] > 5:\n",
    "        score += 10\n",
    "    \n",
    "    # Saltos de fase s√£o suspeitos (NOVA M√âTRICA)\n",
    "    if metrics['phase_jump_rate'] > 5:\n",
    "        score += 15\n",
    "    \n",
    "    # In-phase lag muito baixo ou muito est√°vel pode indicar spoofing\n",
    "    if metrics['in_phase_lag_std'] < 0.01:  # Varia√ß√£o muito pequena\n",
    "        score += 10\n",
    "    \n",
    "    # Largura de correla√ß√£o an√¥mala\n",
    "    if metrics['corr_width_cv'] < 0.05:  # Muito consistente (artificial)\n",
    "        score += 10\n",
    "    \n",
    "    # Alta autocorrela√ß√£o no SNR pode indicar padr√£o artificial\n",
    "    if metrics['snr_autocorr_lag1'] > 0.8:\n",
    "        score += 10\n",
    "    \n",
    "    metrics['spoofing_score'] = min(100, score)\n",
    "    \n",
    "    # Classifica√ß√£o\n",
    "    if metrics['spoofing_score'] < 30:\n",
    "        metrics['classification'] = 'LIKELY AUTHENTIC'\n",
    "    elif metrics['spoofing_score'] < 60:\n",
    "        metrics['classification'] = 'SUSPICIOUS'\n",
    "    else:\n",
    "        metrics['classification'] = 'LIKELY SPOOFED'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_spoofing_report(metrics, filename):\n",
    "    \"\"\"\n",
    "    Imprime um relat√≥rio formatado das m√©tricas de spoofing.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  RELAT√ìRIO DE DETEC√á√ÉO DE SPOOFING - {filename}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüìä ESTAT√çSTICAS DE SNR:\")\n",
    "    print(f\"   M√©dia: {metrics['snr_mean']:.2f}\")\n",
    "    print(f\"   Desvio Padr√£o: {metrics['snr_std']:.2f}\")\n",
    "    print(f\"   Coef. Varia√ß√£o: {metrics['snr_cv']:.3f}\")\n",
    "    print(f\"   M√°x/M√≠n: {metrics['snr_max']:.2f} / {metrics['snr_min']:.2f}\")\n",
    "    print(f\"   % acima do threshold: {metrics['snr_above_threshold']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nüì° AN√ÅLISE DOPPLER:\")\n",
    "    print(f\"   M√©dia: {metrics['doppler_mean']:.1f} Hz\")\n",
    "    print(f\"   Desvio Padr√£o: {metrics['doppler_std']:.1f} Hz\")\n",
    "    print(f\"   Taxa m√©dia de varia√ß√£o: {metrics['doppler_rate_mean']:.2f} Hz/ms\")\n",
    "    print(f\"   Saltos abruptos: {metrics['doppler_jumps']} ({metrics['doppler_jump_rate']:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è AN√ÅLISE DE ATRASO (œÑ):\")\n",
    "    print(f\"   M√©dia: {metrics['tau_mean']:.2f} chips\")\n",
    "    print(f\"   Desvio Padr√£o: {metrics['tau_std']:.2f} chips\")\n",
    "    print(f\"   Taxa m√©dia de varia√ß√£o: {metrics['tau_rate_mean']:.4f} chips/ms\")\n",
    "    \n",
    "    print(\"\\nüõ∞Ô∏è DIVERSIDADE DE PRN:\")\n",
    "    print(f\"   PRNs √∫nicos detectados: {metrics['prn_unique_count']}\")\n",
    "    print(f\"   PRNs: {metrics['prn_unique_list']}\")\n",
    "    print(f\"   PRN dominante: {metrics['prn_dominant']} ({metrics['prn_dominant_ratio']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüìà AN√ÅLISE DE POT√äNCIA:\")\n",
    "    print(f\"   M√©dia: {metrics['power_mean']:.2f}\")\n",
    "    print(f\"   Coef. Varia√ß√£o: {metrics['power_cv']:.3f}\")\n",
    "    \n",
    "    print(\"\\nüìê AN√ÅLISE DE IN-PHASE LAG:\")\n",
    "    print(f\"   M√©dia |lag|: {metrics['in_phase_lag_mean']:.4f} chips\")\n",
    "    print(f\"   Desvio Padr√£o: {metrics['in_phase_lag_std']:.4f} chips\")\n",
    "    print(f\"   M√°ximo |lag|: {metrics['in_phase_lag_max']:.4f} chips\")\n",
    "    print(f\"   Saltos de fase: {metrics['phase_jumps']} ({metrics['phase_jump_rate']:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nüìè LARGURA DE CORRELA√á√ÉO:\")\n",
    "    print(f\"   M√©dia: {metrics['corr_width_mean']:.2f} chips\")\n",
    "    print(f\"   Desvio Padr√£o: {metrics['corr_width_std']:.2f} chips\")\n",
    "    print(f\"   Coef. Varia√ß√£o: {metrics['corr_width_cv']:.3f}\")\n",
    "    \n",
    "    print(\"\\nüîó CORRELA√á√ÉO TEMPORAL:\")\n",
    "    print(f\"   Autocorrela√ß√£o SNR (lag=1): {metrics['snr_autocorr_lag1']:.3f}\")\n",
    "    print(f\"   Autocorrela√ß√£o SNR (lag=10): {metrics['snr_autocorr_lag10']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  üéØ SPOOFING SCORE: {metrics['spoofing_score']}/100\")\n",
    "    print(f\"  üìã CLASSIFICA√á√ÉO: {metrics['classification']}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_spoofing_comparison(results_clean, results_spoofed, metrics_clean, metrics_spoofed):\n",
    "    \"\"\"\n",
    "    Plota compara√ß√£o visual entre sinal limpo e spoofed.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    \n",
    "    # SNR Distribution\n",
    "    axes[0, 0].hist(results_clean['peak_snr'], bins=50, alpha=0.7, label='Clean', color='green')\n",
    "    axes[0, 0].hist(results_spoofed['peak_snr'], bins=50, alpha=0.7, label='Spoofed', color='red')\n",
    "    axes[0, 0].set_xlabel('SNR')\n",
    "    axes[0, 0].set_ylabel('Frequ√™ncia')\n",
    "    axes[0, 0].set_title('Distribui√ß√£o de SNR')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Doppler Distribution\n",
    "    axes[0, 1].hist(results_clean['best_doppler'], bins=50, alpha=0.7, label='Clean', color='green')\n",
    "    axes[0, 1].hist(results_spoofed['best_doppler'], bins=50, alpha=0.7, label='Spoofed', color='red')\n",
    "    axes[0, 1].set_xlabel('Doppler (Hz)')\n",
    "    axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "    axes[0, 1].set_title('Distribui√ß√£o de Doppler')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # PRN Distribution\n",
    "    prn_clean = np.bincount(results_clean['best_prn'].astype(int), minlength=33)[1:]\n",
    "    prn_spoofed = np.bincount(results_spoofed['best_prn'].astype(int), minlength=33)[1:]\n",
    "    x = np.arange(1, 33)\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar(x - width/2, prn_clean, width, label='Clean', color='green', alpha=0.7)\n",
    "    axes[1, 0].bar(x + width/2, prn_spoofed, width, label='Spoofed', color='red', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('PRN')\n",
    "    axes[1, 0].set_ylabel('Contagem')\n",
    "    axes[1, 0].set_title('Distribui√ß√£o de PRN Detectados')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # SNR over time\n",
    "    axes[1, 1].plot(results_clean['time_s'], results_clean['peak_snr'], 'g-', linewidth=0.5, label='Clean', alpha=0.7)\n",
    "    axes[1, 1].plot(results_spoofed['time_s'], results_spoofed['peak_snr'], 'r-', linewidth=0.5, label='Spoofed', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Tempo (s)')\n",
    "    axes[1, 1].set_ylabel('SNR')\n",
    "    axes[1, 1].set_title('SNR ao Longo do Tempo')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    # Spoofing Score Comparison\n",
    "    categories = ['SNR\\nM√©dia', 'SNR\\nCV', 'PRN\\nDiversidade', 'Doppler\\nEstabilidade', 'Score\\nTotal']\n",
    "    clean_scores = [\n",
    "        min(metrics_clean['snr_mean']/20*100, 100),\n",
    "        (1 - min(metrics_clean['snr_cv'], 1)) * 100,\n",
    "        min(metrics_clean['prn_unique_count']/10*100, 100),\n",
    "        max(0, 100 - metrics_clean['doppler_jump_rate']*10),\n",
    "        100 - metrics_clean['spoofing_score']\n",
    "    ]\n",
    "    spoofed_scores = [\n",
    "        min(metrics_spoofed['snr_mean']/20*100, 100),\n",
    "        (1 - min(metrics_spoofed['snr_cv'], 1)) * 100,\n",
    "        min(metrics_spoofed['prn_unique_count']/10*100, 100),\n",
    "        max(0, 100 - metrics_spoofed['doppler_jump_rate']*10),\n",
    "        100 - metrics_spoofed['spoofing_score']\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    axes[2, 0].bar(x - width/2, clean_scores, width, label='Clean', color='green', alpha=0.7)\n",
    "    axes[2, 0].bar(x + width/2, spoofed_scores, width, label='Spoofed', color='red', alpha=0.7)\n",
    "    axes[2, 0].set_ylabel('Score (0-100)')\n",
    "    axes[2, 0].set_title('Compara√ß√£o de M√©tricas')\n",
    "    axes[2, 0].set_xticks(x)\n",
    "    axes[2, 0].set_xticklabels(categories)\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True)\n",
    "    \n",
    "    # Tau variation\n",
    "    axes[2, 1].plot(results_clean['time_s'], results_clean['best_tau_chips'], 'g-', linewidth=0.5, label='Clean', alpha=0.7)\n",
    "    axes[2, 1].plot(results_spoofed['time_s'], results_spoofed['best_tau_chips'], 'r-', linewidth=0.5, label='Spoofed', alpha=0.7)\n",
    "    axes[2, 1].set_xlabel('Tempo (s)')\n",
    "    axes[2, 1].set_ylabel('œÑ (chips)')\n",
    "    axes[2, 1].set_title('Atraso de C√≥digo ao Longo do Tempo')\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Fun√ß√µes de detec√ß√£o de spoofing carregadas!\")\n",
    "print(\"- calculate_spoofing_metrics(results, cfg)\")\n",
    "print(\"- print_spoofing_report(metrics, filename)\")\n",
    "print(\"- plot_spoofing_comparison(results_clean, results_spoofed, metrics_clean, metrics_spoofed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  PROCESSANDO SINAL LIMPO (CLEAN)\n",
      "======================================================================\n",
      "\n",
      "Iniciando aquisi√ß√£o GNSS para o arquivo: /home/rc-2d/Downloads/gnss-dataset/OSNMA_cleandata_opensky_460s.dat\n",
      "\n",
      "Processando 10 blocos de 1.0 ms cada\n",
      "Tempo total: 0.01 segundos\n",
      "Amostras por bloco: 25575\n",
      "\n",
      "Lendo arquivo: /home/rc-2d/Downloads/gnss-dataset/OSNMA_cleandata_opensky_460s.dat\n"
     ]
    }
   ],
   "source": [
    "FILENAME_CLEAN = '/home/rc-2d/Downloads/gnss-dataset/OSNMA_cleandata_opensky_460s.dat'\n",
    "FILENAME_SPOOFED = '/home/rc-2d/Downloads/gnss-dataset/TGS_L1_E1.dat'\n",
    "\n",
    "# Diret√≥rio para salvar os CSVs\n",
    "OUTPUT_DIR = '/home/rc-2d/gnss-spoofing-detection/data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Processar sinal limpo\n",
    "print(\"=\"*70)\n",
    "print(\"  PROCESSANDO SINAL LIMPO (CLEAN)\")\n",
    "print(\"=\"*70)\n",
    "results_clean = acquire_gnss_signal(FILENAME_CLEAN)\n",
    "\n",
    "# 2. Processar sinal spoofed\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  PROCESSANDO SINAL SPOOFED\")\n",
    "print(\"=\"*70)\n",
    "results_spoofed = acquire_gnss_signal(FILENAME_SPOOFED)\n",
    "\n",
    "# 3. Calcular m√©tricas de spoofing\n",
    "if results_clean is not None and results_spoofed is not None:\n",
    "    metrics_clean = calculate_spoofing_metrics(results_clean, PARAMS_CLEAN)\n",
    "    metrics_spoofed = calculate_spoofing_metrics(results_spoofed, PARAMS_DS2)\n",
    "    \n",
    "    # 4. Imprimir relat√≥rios\n",
    "    print_spoofing_report(metrics_clean, \"CLEAN DATA\")\n",
    "    print_spoofing_report(metrics_spoofed, \"SPOOFED DATA\")\n",
    "    \n",
    "    # 5. Plotar compara√ß√£o visual\n",
    "    plot_spoofing_comparison(results_clean, results_spoofed, metrics_clean, metrics_spoofed)\n",
    "    \n",
    "    # 6. Salvar dados em CSV\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Criar DataFrame para dados limpos (label = 0)\n",
    "    df_clean = pd.DataFrame({\n",
    "        'block_idx': results_clean['block_idx'],\n",
    "        'time_s': results_clean['time_s'],\n",
    "        'best_prn': results_clean['best_prn'],\n",
    "        'best_doppler': results_clean['best_doppler'],\n",
    "        'best_tau_chips': results_clean['best_tau_chips'],\n",
    "        'peak_snr': results_clean['peak_snr'],\n",
    "        'peak_raw': results_clean['peak_raw'],\n",
    "        'in_phase_lag': results_clean['in_phase_lag'],\n",
    "        'correlation_width': results_clean['correlation_width'],\n",
    "        'label': 0,  # 0 = aut√™ntico/limpo\n",
    "        'label_name': 'authentic'\n",
    "    })\n",
    "    \n",
    "    # Criar DataFrame para dados spoofed (label = 1)\n",
    "    df_spoofed = pd.DataFrame({\n",
    "        'block_idx': results_spoofed['block_idx'],\n",
    "        'time_s': results_spoofed['time_s'],\n",
    "        'best_prn': results_spoofed['best_prn'],\n",
    "        'best_doppler': results_spoofed['best_doppler'],\n",
    "        'best_tau_chips': results_spoofed['best_tau_chips'],\n",
    "        'peak_snr': results_spoofed['peak_snr'],\n",
    "        'peak_raw': results_spoofed['peak_raw'],\n",
    "        'in_phase_lag': results_spoofed['in_phase_lag'],\n",
    "        'correlation_width': results_spoofed['correlation_width'],\n",
    "        'label': 1,  # 1 = spoofed\n",
    "        'label_name': 'spoofed'\n",
    "    })\n",
    "    \n",
    "    # Salvar CSVs individuais\n",
    "    clean_csv_path = os.path.join(OUTPUT_DIR, 'gnss_clean_samples.csv')\n",
    "    spoofed_csv_path = os.path.join(OUTPUT_DIR, 'gnss_spoofed_samples.csv')\n",
    "    \n",
    "    df_clean.to_csv(clean_csv_path, index=False)\n",
    "    df_spoofed.to_csv(spoofed_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dados limpos salvos em: {clean_csv_path}\")\n",
    "    print(f\"   Amostras: {len(df_clean)}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dados spoofed salvos em: {spoofed_csv_path}\")\n",
    "    print(f\"   Amostras: {len(df_spoofed)}\")\n",
    "    \n",
    "    # Criar dataset combinado para treinamento de ML\n",
    "    df_combined = pd.concat([df_clean, df_spoofed], ignore_index=True)\n",
    "    \n",
    "    # Embaralhar os dados\n",
    "    df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    combined_csv_path = os.path.join(OUTPUT_DIR, 'gnss_dataset_labeled.csv')\n",
    "    df_combined.to_csv(combined_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset combinado salvo em: {combined_csv_path}\")\n",
    "    print(f\"   Total de amostras: {len(df_combined)}\")\n",
    "    print(f\"   - Aut√™nticas: {len(df_clean)} ({len(df_clean)/len(df_combined)*100:.1f}%)\")\n",
    "    print(f\"   - Spoofed: {len(df_spoofed)} ({len(df_spoofed)/len(df_combined)*100:.1f}%)\")\n",
    "    \n",
    "    # Salvar m√©tricas agregadas\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\n",
    "            'source': 'clean',\n",
    "            'label': 0,\n",
    "            **{k: v for k, v in metrics_clean.items() if not isinstance(v, list)}\n",
    "        },\n",
    "        {\n",
    "            'source': 'spoofed', \n",
    "            'label': 1,\n",
    "            **{k: v for k, v in metrics_spoofed.items() if not isinstance(v, list)}\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    metrics_csv_path = os.path.join(OUTPUT_DIR, 'gnss_metrics_summary.csv')\n",
    "    metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ M√©tricas agregadas salvas em: {metrics_csv_path}\")\n",
    "    \n",
    "    # Mostrar preview dos dados\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  PREVIEW DO DATASET COMBINADO\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_combined.head(10).to_string())\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  ESTAT√çSTICAS DO DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_combined.describe().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
